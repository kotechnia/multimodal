# multimodal

## Index
- [Common](#common)
- [Mask2Former - Panoptic Segmentation model](#mask2former)
- [HCRN  - QnA model](#hcrn)
- [LICENSE](#license)

## Contents

â í…ŒìŠ¤íŠ¸ ì‹œìŠ¤í…œ ì‚¬ì–‘ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.
```
Ubuntu 20.04   
Python 3.8.10 
Torch 1.9.0+cu111 
CUDA 11.1
cuDnn 8.2.0   
```
<br>


## Common
> Mask2Formerì™€ HCRNì—ì„œ ê³µí†µëœ í›ˆë ¨, ê²€ì¦, ì‹œí—˜ ì„¸íŠ¸ë¥¼ ì‚¬ìš©í•˜ê¸° ìœ„í•´ ë°ì´í„°ë¥¼ ë¶„í• í•©ë‹ˆë‹¤.
```bash
python prepare.py \
--video_path data/videos \
--segmentation_path data/annotations/segmentation_data.json \
--qna_path data/annotations/QNA_data.json \
--common_split_list common_split_list.json
```
<br/>

## Mask2Former

model : Mask2Former  
config : configs/multimodal/config_multimodal.yaml


â maks2formerì˜ êµ¬ì¡°ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.
```
ğŸ“‚mask2former
â”œâ”€ ğŸ“‚configs
â”‚   â””â”€ ğŸ“‚multimodal
â”œâ”€ ğŸ“‚datasets
â”‚   â””â”€ ğŸ“‚multimodal
â”‚       â”œâ”€ ğŸ“‚annotations
â”‚       â”‚  â”œâ”€ ğŸ“„categories.json
â”‚       â”‚  â”œâ”€ ğŸ“„train.json
â”‚       â”‚  â”œâ”€ ğŸ“„val.json
â”‚       â”‚  â””â”€ ğŸ“„test.json
â”‚       â”œâ”€ ğŸ“‚panoptic_train
â”‚       â”œâ”€ ğŸ“‚panoptic_val
â”‚       â”œâ”€ ğŸ“‚panoptic_test
â”‚       â”œâ”€ ğŸ“‚train
â”‚       â”œâ”€ ğŸ“‚val
â”‚       â””â”€ ğŸ“‚test
â”œâ”€ ğŸ“„README.md
â”œâ”€ ğŸ“„LICENSE
â”œâ”€ ğŸ“„predict.py
â”œâ”€ ğŸ“„requirements.txt
â””â”€ ğŸ“„train_net.py
```
<br>
â ì‚¬ìš© ë¼ì´ë¸ŒëŸ¬ë¦¬ ë° í”„ë¡œê·¸ë¨ì…ë‹ˆë‹¤. 
<br> 


**detectron2**

```
$ git clone https://github.com/facebookresearch/detectron2.git
$ pip install -e detectron2
```

```
$ sed -i s/"int(ann\\['image_id'\\])"/"ann['image_id']"/g detectron2/detectron2/data/datasets/coco_panoptic.py 
```

**panopticapi**
```
$ git clone https://github.com/cocodataset/panopticapi.git
$ pip install -e panopticapi
```

**mask2former**
```
$ cd mask2former
$ pip install -r requirements.txt
$ cd mask2former/modeling/pixel_decoder/ops
$ sh make.sh
$ cd -
```
<br/>

### ë°ì´í„° ì „ì²˜ë¦¬ ë°©ë²• (ì˜ˆì‹œ)
```bash
$ source prepare_multimodal_dataset.sh \
../data/videos/video_frames \
../data/annotations/labels \
../data/annotations/segmentation_data.json \
../data/annotations/categories.json \
../common_split_list.json
```

### ì‹¤í–‰ ë°©ë²• (ì˜ˆì‹œ)

â í›ˆë ¨ ë°©ë²•ì…ë‹ˆë‹¤.
```
$ python train_net.py  \
--config-file configs/multimodal/config_multimodal.yaml \
--num-gpus 2 \
SOLVER.IMS_PER_BATCH 2 \
OUTPUT_DIR ./multimodal
```

â í‰ê°€ ë°©ë²•ì…ë‹ˆë‹¤.
```
$ python train_net.py  \
--config-file multimodal/config.yaml
--num-gpus 2 \
--eval-only \
MODEL.WEIGHTS multimodal/model_final.pth \
DATASETS.EVAl multimodal_2022_test_dataset_panoptic 
```
<br>
<details>
    <summary>â  original github & paper</summary>
    <p>github : <a href='https://github.com/facebookresearch/Mask2Former'>Mask2Former</a>
    <p>paper : <a href='https://arxiv.org/pdf/2112.01527.pdf'>arXiv:2112.01527</a>
</details>

---

## HCRN


model : HCRN  
config (í•œê¸€) : configs/multimodal_qa_action_ko.yml  
config (ì˜ì–´) : configs/multimodal_qa_action_en.yml


â HCRNì˜ êµ¬ì¡°ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.
```
ğŸ“‚cla
â”œâ”€ ğŸ“‚configs
â”‚   â””â”€ ğŸ“„multimodal_qa_action.yml
â”œâ”€ ğŸ“‚data
â”‚   â”œâ”€ ğŸ“‚glove
â”‚   â””â”€ ğŸ“‚multimodal
â”‚       â”œâ”€ ğŸ“‚annotation
â”‚       â”‚   â””â”€ ğŸ“„annotation.csv
â”‚       â””â”€ ğŸ“‚video
â”œâ”€ ğŸ“‚model
â”œâ”€ ğŸ“‚preprocess
â”œâ”€ ğŸ“„.gitignore
â”œâ”€ ğŸ“„CRNUnit.png
â”œâ”€ ğŸ“„DataLoader.py
â”œâ”€ ğŸ“„LICENSE
â”œâ”€ ğŸ“„README.md
â”œâ”€ ğŸ“„config.py
â”œâ”€ ğŸ“„overview.png
â”œâ”€ ğŸ“„requirements.txt
â”œâ”€ ğŸ“„train.py
â”œâ”€ ğŸ“„utils.py
â””â”€ ğŸ“„validate.py
```
<br>

### ë°ì´í„° ì „ì²˜ë¦¬ ë°©ë²• (ì˜ˆì‹œ)


**í•œê¸€**  
```bash
$ python preprocess/make_datasets.py \
--annot_json ../data/annotations/QNA_data.json \
--split_list ../common_split_list.json \
--lang ko

$ python preprocess/preprocess_features.py \
--gpu_id 0 \
--dataset multimodal \
--video_path ../data/videos \
--model resnet101 \
--question_type action

$ python preprocess/preprocess_features.py \
--dataset multimodal \
--video_path ../data/videos \
--model resnext101 \
--image_height 112 \
--image_width 112 \
--question_type action

$ python preprocess/preprocess_questions.py \
--dataset multimodal \
--glove_pt data/glove/glove.768d.ko.pkl \
--question_type action \
--token_type transformers \
--by_video y \
--mode train
```
<br/>


**ì˜ì–´**  
```bash
# config file => train.word_dim: 300
import nltk
nltk.download('punkt')
```
```bash
$ python preprocess/make_datasets.py \
--annot_json ../data/annotations/QNA_data.json \
--split_list ../common_split_list.json \
--lang en

$ python preprocess/preprocess_features.py \
--gpu_id 0 \
--dataset multimodal \
--video_path ../data/videos \
--model resnet101 \
--question_type action

$ python preprocess/preprocess_features.py \
--dataset multimodal \
--video_path ../data/videos \
--model resnext101 \
--image_height 112 \
--image_width 112 \
--question_type action

$ python preprocess/preprocess_questions.py \
--dataset multimodal \
--glove_pt data/glove/glove.840.300d.pkl \
--question_type action \
--token_type transformers \
--by_video y \
--mode train
```
<br/>

### ì‹¤í–‰ ë°©ë²• (ì˜ˆì‹œ)
**í•œê¸€**  
â í›ˆë ¨ ë°©ë²•ì…ë‹ˆë‹¤.
```bash
$ python train.py --cfg configs/multimodal_qa_action_ko.yml
```

â í‰ê°€ ë°©ë²•ì…ë‹ˆë‹¤.
```bash
$ python validate.py --cfg configs/multimodal_qa_action_ko.yml
```
<br/>

**ì˜ì–´**  
â í›ˆë ¨ ë°©ë²•ì…ë‹ˆë‹¤.
```bash
$ python train.py --cfg configs/multimodal_qa_action_en.yml
```

â í‰ê°€ ë°©ë²•ì…ë‹ˆë‹¤.
```bash
$ python validate.py --cfg configs/multimodal_qa_action_en.yml
```



<br>
<details>
    <summary>â  original github & paper</b></summary>
    <p>github : <a href='https://github.com/thaolmk54/hcrn-videoqa'>HCRN</a>
    <p>paper : <a href='https://arxiv.org/pdf/2002.10698.pdf'>arXiv:2002.10698</a>
</details>

---

## License
> The license for this repository is based on the MIT license.   
> If the module has a "LICENSE" file, that license is applied.
