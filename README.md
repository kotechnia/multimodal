# multimodal

## Index
- [Test System Specifications](#â-í…ŒìŠ¤íŠ¸-ì‹œìŠ¤í…œ-ì‚¬ì–‘ì€-ë‹¤ìŒê³¼-ê°™ìŠµë‹ˆë‹¤.)
- [Prepare Environment](#â-ì‚¬ìš©-ë¼ì´ë¸ŒëŸ¬ë¦¬-ë°-í”„ë¡œê·¸ë¨ì…ë‹ˆë‹¤.)
- [Description by Model](#â-ê°-ëª¨ë¸-ë³„-ì„¤ëª…ì…ë‹ˆë‹¤.)
	- [Mask2Former](#<b>Mask2Former</b>) - Panoptic Segmentation model
	- [HCRN](#<b>HCRN</b>)  - QnA model

## Contents

â í…ŒìŠ¤íŠ¸ ì‹œìŠ¤í…œ ì‚¬ì–‘ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.
```
Ubuntu 20.04   
Python 3.8.10 
Torch 1.9.0+cu111 
CUDA 11.1
cuDnn 8.2.0   
```
<br>

â ì‚¬ìš© ë¼ì´ë¸ŒëŸ¬ë¦¬ ë° í”„ë¡œê·¸ë¨ì…ë‹ˆë‹¤.
```bash
# ê³µí†µ
pip install -r requirements.txt

# Mask2Former
pip install -e detectron2
pip install -e panopticapi

cd mask2former/modeling/pixel_decoder/ops
bash make.sh
cd -
```
<br>

â ê° ëª¨ë¸ ë³„ ì„¤ëª…ì…ë‹ˆë‹¤.

<b> Mask2Former</b>

model : Mask2Former  
config : configs/multimodal/config_multimodal.yaml


â maks2formerì˜ êµ¬ì¡°ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.
```
ğŸ“‚mask2former
â”œâ”€ ğŸ“‚configs
â”‚   â”œâ”€ ğŸ“‚coco
â”‚   â””â”€ ğŸ“‚multimodal
â”œâ”€ ğŸ“‚datasets
â”‚   â”œâ”€ ğŸ“‚multimodal
â”‚   â”‚   â”œâ”€ ğŸ“‚annotations
â”‚   â”‚   â”‚  â”œâ”€ ğŸ“„categories.json
â”‚   â”‚   â”‚  â”œâ”€ ğŸ“„train.json
â”‚   â”‚   â”‚  â”œâ”€ ğŸ“„val.json
â”‚   â”‚   â”‚  â””â”€ ğŸ“„test.json
â”‚   â”‚   â”œâ”€ ğŸ“‚panoptic_train
â”‚   â”‚   â”œâ”€ ğŸ“‚panoptic_val
â”‚   â”‚   â”œâ”€ ğŸ“‚panoptic_test
â”‚   â”‚   â”œâ”€ ğŸ“‚train
â”‚   â”‚   â”œâ”€ ğŸ“‚val
â”‚   â”‚   â””â”€ ğŸ“‚test
â”‚   â””â”€ ğŸ“„prepare_coco_semantic_annos_from_panoptic_annos.py
â”œâ”€ ğŸ“‚demo
â”œâ”€ ğŸ“‚demo_video
â”œâ”€ ğŸ“‚mask2former
â”œâ”€ ğŸ“‚mask2former_video
â”œâ”€ ğŸ“‚tools
â”œâ”€ ğŸ“„README.md
â”œâ”€ ğŸ“„LICENSE
â”œâ”€ ğŸ“„predict.py
â”œâ”€ ğŸ“„requirements.txt
â”œâ”€ ğŸ“„train_net.py
â””â”€ ğŸ“„train_net_video.py
```
<br>
â ì‚¬ìš© ë¼ì´ë¸ŒëŸ¬ë¦¬ ë° í”„ë¡œê·¸ë¨ì…ë‹ˆë‹¤.
**detectron2**
```
$ git clone https://github.com/facebookresearch/detectron2.git
$ cd detectron2
$ pip install -e .
$ cd -
```
```
$ sed -i s/"int(ann\\['image_id'\\])"/"ann['image_id']"/g 
```

**panopticapi**
```
$ git clone https://github.com/cocodataset/panopticapi.git
$ cd panopticapi
$ pip install -e .
$ cd -
```

**mask2former**
```
$ cd mask2former
$ pip install -r requirements.txt
```

# ì‹¤í–‰ ë°©ë²• (ì˜ˆì‹œ)

â í›ˆë ¨ ë°©ë²•ì…ë‹ˆë‹¤.
```
python train_net.py  \
--config-file configs/multimodal/config_multimodal.yaml \
--num-gpus 2 \
SOLVER.IMS_PER_BATCH 2 \
OUTPUT_DIR ./<output_dir name>
```

â í‰ê°€ ë°©ë²•ì…ë‹ˆë‹¤.
```
python train_net.py  \
--config-file <output_dir name>/config.yaml
--num-gpus 2 \
--eval-only \
MODEL.WEIGHTS <output_dir name>/checkpoint_file \
DATASETS.EVAl <dataset_name>   # multimodal_2022_test_dataset
```

<details>
    <summary>â  original github & paper</summary>
    <p>github : <a href='https://github.com/facebookresearch/Mask2Former'>Mask2Former</a>
    <p>paper : <a href='https://arxiv.org/pdf/2112.01527.pdf'>arXiv:2112.01527</a>
</details>

---
---

<b>HCRN</b>


model : HCRN  
config : configs/multimodal_qa_action.yml


â HCRNì˜ êµ¬ì¡°ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.
```
ğŸ“‚cla
â”œâ”€ ğŸ“‚configs
â”‚   â””â”€ ğŸ“„multimodal_qa_action.yml
â”œâ”€ ğŸ“‚data
â”‚   â”œâ”€ ğŸ“‚glove
â”‚   â””â”€ ğŸ“‚multimodal
â”‚       â”œâ”€ ğŸ“‚annotation
â”‚       â”‚   â””â”€ ğŸ“„annotation.csv
â”‚       â””â”€ ğŸ“‚video
â”œâ”€ ğŸ“‚model
â”œâ”€ ğŸ“‚preprocess
â”œâ”€ ğŸ“„.gitignore
â”œâ”€ ğŸ“„CRNUnit.png
â”œâ”€ ğŸ“„DataLoader.py
â”œâ”€ ğŸ“„LICENSE
â”œâ”€ ğŸ“„README.md
â”œâ”€ ğŸ“„config.py
â”œâ”€ ğŸ“„overview.png
â”œâ”€ ğŸ“„requirements.txt
â”œâ”€ ğŸ“„train.py
â”œâ”€ ğŸ“„utils.py
â””â”€ ğŸ“„validate.py
```
<br>

### ë°ì´í„° ì „ì²˜ë¦¬ ë°©ë²• (ì˜ˆì‹œ)

â ë¹„ë””ì˜¤ ì „ì²˜ë¦¬ ë°©ë²•ì…ë‹ˆë‹¤.
```bash
python preprocess/preprocess_features.py \
--gpu_id 0 \
--dataset multimodal \
--model resnet101 \
--question_type action

python preprocess/preprocess_features.py \
--dataset multimodal \
--model resnext101 \
--image_height 112 \
--image_width 112 \
--question_type action
```

â annotation ì „ì²˜ë¦¬ ë°©ë²•ì…ë‹ˆë‹¤.
```bash
python preprocess/preprocess_questions.py \
--dataset multimodal \
--glove_pt data/glove/glove.840.300d.pkl \
--question_type action \
--token_type transformers \
--tokenizer tokenizer/my_tokenizer \
--by_video y \
--mode train

```


### ì‹¤í–‰ ë°©ë²• (ì˜ˆì‹œ)

â í›ˆë ¨ ë°©ë²•ì…ë‹ˆë‹¤.
```bash
python train.py --cfg configs/multimodal_qa_action.yml
```

â í‰ê°€ ë°©ë²•ì…ë‹ˆë‹¤.
```bash
python validate.py --cfg configs/multimodal_qa_action.yml
```

<details>
    <summary>â  original github & paper</b></summary>
    <p>github : <a href='https://github.com/thaolmk54/hcrn-videoqa'>HCRN</a>
    <p>paper : <a href='https://arxiv.org/pdf/2002.10698.pdf'>arXiv:2002.10698</a>
</details>

